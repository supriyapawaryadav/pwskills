{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ddfa013",
   "metadata": {},
   "source": [
    "# 1.What is a parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e5fcb",
   "metadata": {},
   "source": [
    "A parameter is a numerical value that describes a characteristic or feature of a population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c31c7",
   "metadata": {},
   "source": [
    "# 2.What is correlation? What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f1a2c9",
   "metadata": {},
   "source": [
    "Correlation refers to a statistical measure that describes the strength and direction of the relationship between two variables. It helps us understand whether and how changes in one variable are associated with changes in another\n",
    "\n",
    "Negative Correlation:\n",
    "\n",
    "When one variable increases, the other variable tends to decrease.\n",
    "For example, the more hours you spend watching TV, the less time you may spend studying.\n",
    "The correlation coefficient for a negative correlation is between 0 and -1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2f458",
   "metadata": {},
   "source": [
    "# 3.Define Machine Learning. What are the main components in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe00880",
   "metadata": {},
   "source": [
    "Machine Learning (ML) is a subset of artificial intelligence (AI) that enables systems to learn from data, identify patterns, and make decisions or predictions without being explicitly programmed\n",
    "\n",
    "Main components in Machine Learning;-\n",
    "\n",
    "Data,Features (Feature Engineering), Algorithms ,Model,Training,Evaluation,Hyperparameters,Deployment,Feedback Loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76240d99",
   "metadata": {},
   "source": [
    "# 4.How does loss value help in determining whether the model is good or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8401be",
   "metadata": {},
   "source": [
    "The **loss value** is a numerical measure that evaluates how well or poorly a machine learning model performs on a given dataset. It quantifies the difference between the predicted outputs of the model and the actual target values. The primary role of the loss value is to guide the training process by helping the model improve its predictions.\n",
    "\n",
    "---\n",
    "\n",
    " **How Loss Value Helps in Determining Model Quality**\n",
    "\n",
    "1. **Lower Loss Value = Better Model Predictions**  \n",
    "   - A low loss value indicates that the model's predictions are close to the actual target values, meaning the model is performing well.  \n",
    "   - A high loss value suggests the model's predictions deviate significantly from the true labels, which means the model needs improvement.\n",
    "\n",
    "2. **Optimization Objective**  \n",
    "   - Most machine learning models are trained to minimize the loss function. During training, the optimization algorithm (e.g., gradient descent) adjusts the model's parameters (weights) to reduce the loss.  \n",
    "   - A consistent decrease in the loss value during training shows that the model is learning effectively.\n",
    "\n",
    "3. **Overfitting/Underfitting**  \n",
    "   - If the **training loss** is low, but the **validation loss** is high, the model might be overfitting (memorizing the training data but not generalizing well to new data).  \n",
    "   - If both training and validation losses remain high, the model might be underfitting (too simple to capture the data's complexity).  \n",
    "   - Analyzing the loss values helps identify these issues.\n",
    "\n",
    "4. **Choice of Loss Function**  \n",
    "   - The loss function depends on the problem type:\n",
    "     - **Regression**: Mean Squared Error (MSE), Mean Absolute Error (MAE).  \n",
    "     - **Classification**: Cross-Entropy Loss, Hinge Loss.  \n",
    "     - The choice of the right loss function directly affects the quality of the model.\n",
    "\n",
    "\n",
    "\n",
    "### **Key Metrics vs Loss**\n",
    "- The loss value alone is not always sufficient to determine if the model is \"good.\"  \n",
    "- Metrics like **accuracy**, **precision**, **recall**, or **F1-score** should also be evaluated, especially for real-world applications where specific criteria matter.  \n",
    "- A \"good\" model balances a low loss with high performance on these metrics.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5714d2f7",
   "metadata": {},
   "source": [
    "# 5.What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1c7d54",
   "metadata": {},
   "source": [
    "1.Continuous Variables\n",
    "\n",
    "Definition: Continuous variables are numeric variables that can take an infinite number of values within a given range. They are measured on a continuous scale and often represent quantities or measurements.\n",
    "\n",
    "Requires encoding (e.g., one-hot encoding, label encoding) to convert into numerical format.\n",
    "\n",
    "Used in regression problems.\n",
    "Requires normalization/scaling for some algorithms.\n",
    "\n",
    "example - Height (e.g., 5.8 feet)\n",
    "\n",
    "2.Categorical Variables\n",
    "Definition: Categorical variables are variables that represent groups or categories. They are non-numeric and often used to label data.\n",
    "\n",
    "Types:\n",
    "\n",
    "Nominal: Categories with no inherent order (e.g., color: red, blue, green).\n",
    "\n",
    "Ordinal: Categories with a meaningful order (e.g., rating: poor, average, good).\n",
    "\n",
    "Examples: Gender (e.g., Male, Female, Other)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc470c28",
   "metadata": {},
   "source": [
    "# 6.How do we handle categorical variables in Machine Learning? What are the common techniques?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de77c57",
   "metadata": {},
   "source": [
    "1. Label Encoding\n",
    "\n",
    "Converts each category into a unique integer.\n",
    "\n",
    "Use Case:\n",
    "\n",
    "Suitable for ordinal variables (categories with a meaningful order, e.g., \"low < medium < high\").\n",
    "\n",
    "Limitation:\n",
    "\n",
    "Not ideal for nominal data, as algorithms might mistakenly assume a relationship/order between values.\n",
    "\n",
    "example:\n",
    "\n",
    "Color: [Red, Blue, Green] → [0, 1, 2]\n",
    "\n",
    "2. One-Hot Encoding\n",
    "\n",
    "Converts categories into binary columns (0s and 1s), creating a separate column for each category.\n",
    "\n",
    "Use Case:\n",
    "\n",
    "Suitable for nominal variables (categories with no inherent order).\n",
    "\n",
    "Limitation:\n",
    "\n",
    "May lead to a \"curse of dimensionality\" if the variable has many categories.\n",
    "\n",
    "Color: [Red, Blue, Green] →  \n",
    "Red   Blue   Green  \n",
    "1      0      0  \n",
    "0      1      0  \n",
    "0      0      1  \n",
    "\n",
    "\n",
    "3. Target Encoding (Mean Encoding)\n",
    "\n",
    "Replaces categories with the mean of the target variable for each category.\n",
    "\n",
    "Example (for a binary classification task):\n",
    "\n",
    "Category: [A, B, C]  \n",
    "Target: [1, 0, 1]  \n",
    "→ A: 0.67, B: 0.0, C: 1.0\n",
    "\n",
    "Use Case:\n",
    "\n",
    "Useful when categories have a significant relationship with the target.\n",
    "\n",
    "Limitation:\n",
    "\n",
    "Risk of data leakage if not applied carefully (e.g., on the test set).\n",
    "\n",
    "4. Frequency Encoding\n",
    "\n",
    "Encodes categories based on their frequency of occurrence in the dataset.\n",
    "\n",
    "Example:\n",
    "\n",
    "Fruit: [Apple, Orange, Apple, Banana] →  \n",
    "Apple: 2, Orange: 1, Banana: 1\n",
    "\n",
    "Use Case:\n",
    "\n",
    "Useful for high-cardinality categorical features.\n",
    "\n",
    "5. Binary Encoding\n",
    "\n",
    "Combines one-hot encoding and label encoding by converting categories to binary numbers and then splitting each binary digit into a separate column.\n",
    "\n",
    "Example:\n",
    "\n",
    "Category: [A, B, C] → Label Encoding → [1, 2, 3] → Binary → [01, 10, 11]\n",
    "\n",
    "Use Case:\n",
    "\n",
    "Useful for high-cardinality features to reduce dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0c4255",
   "metadata": {},
   "source": [
    "# 7.What do you mean by training and testing a dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe58ccc4",
   "metadata": {},
   "source": [
    "Training Dataset:\n",
    "\n",
    "The subset of data used to train a machine learning model. The model learns patterns, relationships, and rules in this phase by adjusting its parameters.\n",
    "\n",
    "Testing Dataset:\n",
    "\n",
    "A separate subset of data not seen by the model during training. It is used to evaluate how well the trained model performs on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f616daba",
   "metadata": {},
   "source": [
    "# 8.What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335b74e",
   "metadata": {},
   "source": [
    "sklearn.preprocessing is a module in Scikit-learn that provides a collection of tools for preprocessing data. Preprocessing is a critical step in the machine learning pipeline, as it involves transforming raw data into a format that is suitable for training a machine learning model.\n",
    "\n",
    "The sklearn.preprocessing module includes functions and classes for scaling, normalization, encoding categorical features, and generating polynomial features, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa968b19",
   "metadata": {},
   "source": [
    "# 9.What is a Test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc26a1a8",
   "metadata": {},
   "source": [
    "A test set is a subset of data used in machine learning to evaluate the performance of a trained model. It consists of data that the model has never seen during training, ensuring that the evaluation reflects how well the model generalizes to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f309e",
   "metadata": {},
   "source": [
    "# 10.How do we split data for model fitting (training and testing) in Python?How do you approach a Machine Learning problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b32098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      " [[ 9 10]\n",
      " [ 5  6]\n",
      " [ 1  2]\n",
      " [ 7  8]]\n",
      "Testing Features:\n",
      " [[3 4]]\n",
      "Training Labels:\n",
      " [0 0 0 1]\n",
      "Testing Labels:\n",
      " [1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])  # Features\n",
    "y = np.array([0, 1, 0, 1, 0])  # Target labels\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training Features:\\n\", X_train)\n",
    "print(\"Testing Features:\\n\", X_test)\n",
    "print(\"Training Labels:\\n\", y_train)\n",
    "print(\"Testing Labels:\\n\", y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839cfcde",
   "metadata": {},
   "source": [
    "approach a Machine Learning problem-\n",
    "\n",
    "Step 1: Define the Problem\n",
    "\n",
    "Step 2: Collect and Explore the Data\n",
    "\n",
    "Step 3: Preprocess the Data\n",
    "\n",
    "Step 4: Split Data\n",
    "\n",
    "Step 5: Select and Train a Model\n",
    "\n",
    "Step 6: Evaluate the Model\n",
    "\n",
    "Step 7: Hyperparameter Tuning\n",
    "\n",
    "Step 8: Test on the Test Set\n",
    "\n",
    "Step 9: Deploy the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94232ed1",
   "metadata": {},
   "source": [
    "# 11.Why do we have to perform EDA before fitting a model to the data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636ac28f",
   "metadata": {},
   "source": [
    "EDA helps you understand your data, detect issues, and make informed decisions for preprocessing and modeling. Skipping EDA can lead to poor model performance, biased results, or incorrect insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d5cf39",
   "metadata": {},
   "source": [
    "# 12.What is correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb4c645",
   "metadata": {},
   "source": [
    "Correlation is a statistical measure that describes the strength and direction of a relationship between two variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3e3019",
   "metadata": {},
   "source": [
    "# 13.What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e327c7",
   "metadata": {},
   "source": [
    "Negative correlation means that as one variable increases, the other decreases, or vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88a2974",
   "metadata": {},
   "source": [
    "# 14.How can you find correlation between variables in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75cb45d",
   "metadata": {},
   "source": [
    "find the correlation between variables using the pandas library, which provides a simple method for calculating correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69e7e76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Variable1  Variable2\n",
      "Variable1        1.0       -1.0\n",
      "Variable2       -1.0        1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Creating a sample dataset\n",
    "data = {'Variable1': [1, 2, 3, 4, 5],\n",
    "        'Variable2': [5, 4, 3, 2, 1]}\n",
    "df = pd.DataFrame(data)\n",
    "correlation = df.corr()\n",
    "print(correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f817ef25",
   "metadata": {},
   "source": [
    "# 15.What is causation? Explain difference between correlation and causation with an example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb700d1",
   "metadata": {},
   "source": [
    "Causation refers to a cause-and-effect relationship between two variables, where one variable directly causes the change in another. In other words, a change in one variable directly leads to a change in another variable.\n",
    "\n",
    "Difference Between Correlation and Causation:\n",
    "\n",
    "Correlation: When two variables are correlated, it means there is a statistical relationship between them, but it doesn’t necessarily mean that one variable causes the other to change. Correlation only indicates that the variables tend to change together in some way.\n",
    "\n",
    "Causation: Causation, on the other hand, means that a change in one variable directly causes a change in another. For causation to exist, there must be a clear mechanism or reason why one variable affects the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe06b0",
   "metadata": {},
   "source": [
    "# 16.What is an Optimizer? What are different types of optimizers? Explain each with an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d297aa2d",
   "metadata": {},
   "source": [
    "An optimizer is a key component in machine learning and deep learning algorithms that helps minimize the loss function (or cost function) to improve the model's accuracy. The primary role of an optimizer is to adjust the model's parameters (weights) during training to minimize the error between predicted and actual values, effectively improving the model's performance.\n",
    "\n",
    "Different Types of Optimizers:\n",
    "\n",
    "Gradient Descent (GD)\n",
    "\n",
    "Gradient Descent is the most common optimization algorithm. It updates the parameters (weights) of the model by moving in the direction of the negative gradient of the loss function with respect to the parameters.\n",
    "\n",
    "Example: Suppose you're training a simple linear regression model, and you use Gradient Descent to find the best fit line. The optimizer will iteratively adjust the slope and intercept of the line to minimize the difference between the predicted and actual values (the loss).\n",
    "Types of Gradient Descent:\n",
    "\n",
    "Batch Gradient Descent: It computes the gradient using the entire dataset. While it’s accurate, it can be computationally expensive with large datasets.\n",
    "Stochastic Gradient Descent (SGD): It updates the parameters using a single data point at a time. It’s faster but can be noisier.\n",
    "\n",
    "Mini-batch Gradient Descent: A compromise between batch and stochastic, where the dataset is divided into smaller batches for \n",
    "parameter updates.\n",
    "\n",
    "Stochastic Gradient Descent (SGD)\n",
    "\n",
    "In SGD, the model's parameters are updated for each training example (data point) individually, rather than using the whole dataset. This can lead to faster convergence but can have more fluctuation during training.\n",
    "\n",
    "Example: When training a neural network on an image dataset, using SGD, the parameters of the network are updated after processing each individual image.\n",
    "\n",
    "Momentum\n",
    "\n",
    "Momentum improves the convergence speed by adding a fraction of the previous update to the current update. This helps the optimizer avoid getting stuck in local minima and speeds up convergence, especially in regions with flat gradients.\n",
    "\n",
    "Example: In training a neural network, if the optimizer is moving slowly, momentum can accelerate the movement towards the global minimum, making the learning process faster.\n",
    "\n",
    "RMSprop (Root Mean Square Propagation)\n",
    "\n",
    "RMSprop is an adaptive learning rate method. It divides the learning rate by a moving average of the recent magnitudes of the gradients. This helps in dealing with issues where the gradient's magnitude is very small or very large.\n",
    "\n",
    "Example: In training deep networks on data with highly varying feature scales, RMSprop adjusts the learning rate automatically, making the training more stable.\n",
    "\n",
    "Adam (Adaptive Moment Estimation)\n",
    "\n",
    "Adam is a widely used optimizer that combines the benefits of both Momentum and RMSprop. It computes adaptive learning rates for each parameter by keeping track of both the first moment (mean) and second moment (variance) of the gradients.\n",
    "\n",
    "Example: When training a recurrent neural network (RNN), Adam can be used to adaptively adjust the learning rate for different weights, improving convergence speed and stability.\n",
    "\n",
    "Adagrad (Adaptive Gradient Algorithm)\n",
    "\n",
    "Adagrad adjusts the learning rate for each parameter individually based on its historical gradient. This can be helpful for sparse data but might lead to very small learning rates after many iterations.\n",
    "\n",
    "Example: When training a model on a sparse dataset, such as text data where most words don't appear frequently, Adagrad will increase the learning rate for infrequent features, helping the model adapt better.\n",
    "\n",
    "Adadelta\n",
    "\n",
    "Adadelta is an extension of Adagrad that aims to fix its aggressive, monotonically decreasing learning rates. It uses a moving average of squared gradients to scale the learning rate, making it more adaptive and stable.\n",
    "\n",
    "Example: Adadelta is often used in deep neural networks, where it can effectively adapt the learning rate based on the gradient history, making it more robust than Adagrad.\n",
    "\n",
    "Nadam (Nesterov-accelerated Adaptive Moment Estimation)\n",
    "\n",
    "Nadam is a combination of Adam and Nesterov momentum. It incorporates the Nesterov accelerated gradient into Adam, allowing for better performance in some cases.\n",
    "\n",
    "Example: Nadam is frequently used for training complex deep learning models, such as convolutional neural networks (CNNs), where it can provide faster convergence compared to other optimizers like Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aefe1c0",
   "metadata": {},
   "source": [
    "# 17.What is sklearn.linear_model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12bb64c",
   "metadata": {},
   "source": [
    "sklearn.linear_model is a module in the scikit-learn library that provides a variety of linear models for regression and classification tasks. Linear models are a class of algorithms that assume a linear relationship between the input features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f3b9cf",
   "metadata": {},
   "source": [
    "# 18.What does model.fit() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1739529f",
   "metadata": {},
   "source": [
    "The model.fit() method in scikit-learn is used to train a machine learning model. It adjusts the model parameters (like weights and biases) to fit the training data provided. This is the step where the model learns from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa13a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient (slope): [1.5]\n",
      "Intercept: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Features (X) and target (y)\n",
    "X = [[1], [2], [3], [4]]\n",
    "y = [2.5, 4.0, 5.5, 7.0]\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# View learned parameters\n",
    "print(\"Coefficient (slope):\", model.coef_)  # [1.5]\n",
    "print(\"Intercept:\", model.intercept_)      # 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960a683d",
   "metadata": {},
   "source": [
    "# 19.What does model.predict() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4895730e",
   "metadata": {},
   "source": [
    "The model.predict() method in scikit-learn is used to make predictions based on a trained model. After the model has been trained using model.fit(), you can use model.predict() to generate predictions for new or unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26a62cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [ 8.5 10. ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Training data\n",
    "X_train = [[1], [2], [3], [4]]\n",
    "y_train = [2.5, 4.0, 5.5, 7.0]\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# New data for prediction\n",
    "X_test = [[5], [6]]\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87219140",
   "metadata": {},
   "source": [
    "# 20.What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8413dfcf",
   "metadata": {},
   "source": [
    "1.Continuous Variables\n",
    "\n",
    "Definition: Continuous variables are numeric variables that can take an infinite number of values within a given range. They are measured on a continuous scale and often represent quantities or measurements.\n",
    "\n",
    "Requires encoding (e.g., one-hot encoding, label encoding) to convert into numerical format.\n",
    "\n",
    "Used in regression problems.\n",
    "Requires normalization/scaling for some algorithms.\n",
    "\n",
    "example - Height (e.g., 5.8 feet)\n",
    "\n",
    "2.Categorical Variables\n",
    "Definition: Categorical variables are variables that represent groups or categories. They are non-numeric and often used to label data.\n",
    "\n",
    "Types:\n",
    "\n",
    "Nominal: Categories with no inherent order (e.g., color: red, blue, green).\n",
    "\n",
    "Ordinal: Categories with a meaningful order (e.g., rating: poor, average, good).\n",
    "\n",
    "Examples: Gender (e.g., Male, Female, Other)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe27f2a",
   "metadata": {},
   "source": [
    "# 21.What is feature scaling? How does it help in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a577a3ec",
   "metadata": {},
   "source": [
    "Feature scaling is the process of normalizing or standardizing the range of independent variables (features) in a dataset. It ensures that all features contribute equally to the model's predictions and helps algorithms perform better.\n",
    "\n",
    "Improves Model Performance:\n",
    "\n",
    "Prevents Bias:\n",
    "\n",
    "Reduces Training Time:\n",
    "\n",
    "Ensures Compatibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "262af489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Features:\n",
      " [[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "X = [[150, 70],\n",
    "     [160, 80],\n",
    "     [170, 90]]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Scaled Features:\\n\", X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e3f070",
   "metadata": {},
   "source": [
    "# 22.How do we perform scaling in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605822c8",
   "metadata": {},
   "source": [
    "feature scaling is typically performed using libraries like scikit-learn, which provides convenient tools for various scaling methods such as standardization, min-max scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4477620",
   "metadata": {},
   "source": [
    "Standardization (Z-Score Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bb8683e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized Data:\n",
      " [[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example dataset\n",
    "X = [[1, 200], [2, 300], [3, 400]]\n",
    "\n",
    "# Create the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Standardized Data:\\n\", X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d349ed3",
   "metadata": {},
   "source": [
    "Min-Max Scaling (Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79c9557f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Scaled Data:\n",
      " [[0.  0. ]\n",
      " [0.5 0.5]\n",
      " [1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example dataset\n",
    "X = [[1, 200], [2, 300], [3, 400]]\n",
    "\n",
    "# Create the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Min-Max Scaled Data:\\n\", X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095e25af",
   "metadata": {},
   "source": [
    "Max Absolute Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "380355e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Absolute Scaled Data:\n",
      " [[ 0.33333333 -0.5       ]\n",
      " [ 0.66666667  0.75      ]\n",
      " [ 1.         -1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "# Example dataset\n",
    "X = [[1, -200], [2, 300], [3, -400]]\n",
    "\n",
    "# Create the scaler\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Max Absolute Scaled Data:\\n\", X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffc3647",
   "metadata": {},
   "source": [
    "Robust Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58f18908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust Scaled Data:\n",
      " [[-1.   -0.25]\n",
      " [ 0.    0.  ]\n",
      " [ 1.    1.75]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Example dataset\n",
    "X = [[1, 200], [2, 300], [3, 1000]]  # 1000 is an outlier\n",
    "\n",
    "# Create the scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Robust Scaled Data:\\n\", X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39386a07",
   "metadata": {},
   "source": [
    "# 23.What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dfc1de",
   "metadata": {},
   "source": [
    "sklearn.preprocessing is a module in the scikit-learn library that provides tools for preprocessing and transforming data before feeding it into a machine learning model. Preprocessing ensures that the data is in the right format, scale, and distribution, which can improve model performance and accuracy.\n",
    "\n",
    "The module includes techniques for scaling, normalizing, encoding, and imputing missing values, among other preprocessing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56324fc8",
   "metadata": {},
   "source": [
    "# 24.How do we split data for model fitting (training and testing) in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343451cd",
   "metadata": {},
   "source": [
    "Steps to Split Data\n",
    "\n",
    "1.Import the Required Library: Use train_test_split from sklearn.model_selection.\n",
    "\n",
    "2.Split the Dataset: Specify the proportion of data for training and testing.\n",
    "\n",
    "3.Fit the Model on the Training Set: Train the model using the training data.\n",
    "\n",
    "4.Evaluate on the Testing Set: Test the model's performance on unseen data using the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbd1d7d",
   "metadata": {},
   "source": [
    "# 25.Explain data encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e9da31",
   "metadata": {},
   "source": [
    "Data encoding is the process of converting categorical data (non-numerical values) into numerical representations so that machine learning algorithms can process them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
